{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8be5b3dc",
   "metadata": {},
   "source": [
    "### CRNN Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c90ed6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load training and test data\n",
    "with open('data/train_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Extract data components\n",
    "X_train_mfcc = data['X_train_mfcc']\n",
    "X_train_prosodic = data['X_train_prosodic']\n",
    "X_test_mfcc = data['X_test_mfcc']\n",
    "X_test_prosodic = data['X_test_prosodic']\n",
    "y_train = data['y_train']\n",
    "y_test = data['y_test']\n",
    "encoder = data['encoder']\n",
    "feature_names = data['feature_names']\n",
    "\n",
    "print(\"Data successfully loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9c1646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "# Import necessary libraries\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bacb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors for GPU processing\n",
    "X_train_mfcc_tensor = torch.FloatTensor(X_train_mfcc)\n",
    "X_train_prosodic_tensor = torch.FloatTensor(X_train_prosodic)\n",
    "X_test_mfcc_tensor = torch.FloatTensor(X_test_mfcc)\n",
    "X_test_prosodic_tensor = torch.FloatTensor(X_test_prosodic)\n",
    "\n",
    "y_train_tensor = torch.LongTensor(y_train)\n",
    "y_test_tensor = torch.LongTensor(y_test)\n",
    "\n",
    "print(\"Tensors created successfully!\")\n",
    "print(f\"MFCC tensor shape: {X_train_mfcc_tensor.shape}\")\n",
    "print(f\"Prosodic tensor shape: {X_train_prosodic_tensor.shape}\")\n",
    "print(f\"Labels tensor shape: {y_train_tensor.shape}\")\n",
    "\n",
    "# Create initial data loaders for shape verification\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_dataset = TensorDataset(X_train_mfcc_tensor, X_train_prosodic_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_mfcc_tensor, X_test_prosodic_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d9d5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFCC shape: torch.Size([1152, 1, 128, 345])\n",
      "Prosodic features shape: torch.Size([1152, 9])\n",
      "Number of training batches: 36\n",
      "Number of test batches: 9\n"
     ]
    }
   ],
   "source": [
    "# Print shapes to verify\n",
    "print(\"MFCC shape:\", X_train_mfcc_tensor.shape)\n",
    "print(\"Prosodic features shape:\", X_train_prosodic_tensor.shape)\n",
    "print(\"Number of training batches:\", len(train_loader))\n",
    "print(\"Number of test batches:\", len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c505de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class MultiBranchCRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Branch CRNN for Speech Emotion Recognition\n",
    "    - MFCC Branch: CNN feature extraction + RNN temporal modeling\n",
    "    - Prosodic Branch: Dense layers for prosodic features\n",
    "    - Prosody-Aware Attention: Uses prosodic features to guide attention\n",
    "    \"\"\"\n",
    "    def __init__(self, mfcc_freq_dim, prosody_dim, num_classes, \n",
    "                 cnn_channels=[64, 128, 256], rnn_hidden_size=256, \n",
    "                 rnn_layers=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        print(\"Initializing Multi-Branch CRNN Architecture...\")\n",
    "        \n",
    "        # MFCC Branch: CNN Feature Extractor\n",
    "        # Processes spectral features with increasing channel depth\n",
    "        self.mfcc_cnn = nn.Sequential(\n",
    "            # First CNN block: Extract low-level spectral patterns\n",
    "            nn.Conv2d(1, cnn_channels[0], kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(cnn_channels[0]),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(dropout * 0.5),\n",
    "            \n",
    "            # Second CNN block: Extract mid-level features with pooling\n",
    "            nn.Conv2d(cnn_channels[0], cnn_channels[1], kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(cnn_channels[1]),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2)),  # Reduce frequency and time dimensions\n",
    "            nn.Dropout2d(dropout * 0.7),\n",
    "            \n",
    "            # Third CNN block: Extract high-level features\n",
    "            nn.Conv2d(cnn_channels[1], cnn_channels[2], kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(cnn_channels[2]),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=(2, 1)),  # Reduce only frequency dimension\n",
    "            nn.Dropout2d(dropout),\n",
    "            \n",
    "            # Adaptive pooling to standardize frequency dimension\n",
    "            nn.AdaptiveAvgPool2d((1, None))  # Pool to 1 frequency bin, keep time\n",
    "        )\n",
    "        \n",
    "        # Calculate CNN output dimension automatically\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 1, mfcc_freq_dim, 100)\n",
    "            cnn_output = self.mfcc_cnn(dummy_input)\n",
    "            self.cnn_output_dim = cnn_output.size(1)\n",
    "            print(f\"CNN output dimension: {self.cnn_output_dim}\")\n",
    "        \n",
    "        # Bidirectional LSTM for temporal modeling\n",
    "        # Captures both forward and backward temporal dependencies\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.cnn_output_dim,\n",
    "            hidden_size=rnn_hidden_size // 2,  # Divide by 2 for bidirectional\n",
    "            num_layers=rnn_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if rnn_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # Prosodic Branch: Dense layers for prosodic features\n",
    "        # Processes global acoustic features like pitch, energy, rhythm\n",
    "        self.prosody_branch = nn.Sequential(\n",
    "            nn.Linear(prosody_dim, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout * 0.5)\n",
    "        )\n",
    "        \n",
    "        # Multi-head Attention configuration\n",
    "        self.attention_heads = 8\n",
    "        self.head_dim = rnn_hidden_size // self.attention_heads\n",
    "        \n",
    "        # Attention projection layers\n",
    "        self.query_proj = nn.Linear(rnn_hidden_size, rnn_hidden_size)\n",
    "        self.key_proj = nn.Linear(rnn_hidden_size, rnn_hidden_size)\n",
    "        self.value_proj = nn.Linear(rnn_hidden_size, rnn_hidden_size)\n",
    "        self.attention_out = nn.Linear(rnn_hidden_size, rnn_hidden_size)\n",
    "        \n",
    "        # Prosody-aware attention weights\n",
    "        # Uses prosodic features to modulate attention weights\n",
    "        self.prosody_attention = nn.Sequential(\n",
    "            nn.Linear(64 + rnn_hidden_size, rnn_hidden_size // 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(rnn_hidden_size // 2, self.attention_heads),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        # Final classifier: Combines MFCC and prosodic features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(rnn_hidden_size + 64, rnn_hidden_size),\n",
    "            nn.LayerNorm(rnn_hidden_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Linear(rnn_hidden_size, rnn_hidden_size // 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout * 0.5),\n",
    "            \n",
    "            nn.Linear(rnn_hidden_size // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "        self._initialize_weights()\n",
    "        print(\"Multi-Branch CRNN initialized successfully!\")\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize model weights for stable training\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "            elif isinstance(module, nn.LSTM):\n",
    "                for name, param in module.named_parameters():\n",
    "                    if 'weight' in name:\n",
    "                        nn.init.xavier_uniform_(param)\n",
    "                    elif 'bias' in name:\n",
    "                        nn.init.constant_(param, 0)\n",
    "    \n",
    "    def prosody_aware_attention(self, lstm_output, prosodic_features):\n",
    "        \"\"\"\n",
    "        Prosody-Aware Multi-Head Attention Mechanism\n",
    "        Uses prosodic features to guide attention weights on LSTM output\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, hidden_size = lstm_output.shape\n",
    "        \n",
    "        # Multi-head attention projections\n",
    "        Q = self.query_proj(lstm_output).view(batch_size, seq_len, self.attention_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.key_proj(lstm_output).view(batch_size, seq_len, self.attention_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.value_proj(lstm_output).view(batch_size, seq_len, self.attention_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        # Prosody-aware modulation\n",
    "        # Combine prosodic features with mean LSTM output for context\n",
    "        lstm_mean = lstm_output.mean(dim=1, keepdim=True)  # Global context\n",
    "        prosody_lstm_combined = torch.cat([\n",
    "            prosodic_features.unsqueeze(1).expand(-1, seq_len, -1),\n",
    "            lstm_mean.expand(-1, seq_len, -1)\n",
    "        ], dim=-1)\n",
    "        \n",
    "        # Generate prosody-aware attention modulation weights\n",
    "        prosody_weights = self.prosody_attention(prosody_lstm_combined)  # [B, T, num_heads]\n",
    "        prosody_weights = prosody_weights.transpose(1, 2).unsqueeze(-1)  # [B, num_heads, T, 1]\n",
    "        \n",
    "        # Apply prosody modulation to attention weights\n",
    "        attention_weights = attention_weights * prosody_weights\n",
    "        attention_weights = F.softmax(attention_weights, dim=-1)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        attended_output = torch.matmul(attention_weights, V)\n",
    "        attended_output = attended_output.transpose(1, 2).contiguous().view(batch_size, seq_len, hidden_size)\n",
    "        \n",
    "        # Residual connection and output projection\n",
    "        attended_output = self.attention_out(attended_output)\n",
    "        return lstm_output + attended_output  # Residual connection\n",
    "    \n",
    "    def forward(self, mfcc_features, prosodic_features):\n",
    "        \"\"\"\n",
    "        Forward pass through Multi-Branch CRNN\n",
    "        \"\"\"\n",
    "        # Ensure MFCC features have channel dimension for CNN\n",
    "        if mfcc_features.dim() == 3:\n",
    "            mfcc_features = mfcc_features.unsqueeze(1)  # Add channel dimension [B, 1, F, T]\n",
    "        \n",
    "        # MFCC Branch: CNN feature extraction\n",
    "        cnn_features = self.mfcc_cnn(mfcc_features)  # [B, C, 1, T]\n",
    "        cnn_features = cnn_features.squeeze(2).transpose(1, 2)  # [B, T, C]\n",
    "        \n",
    "        # LSTM temporal modeling\n",
    "        lstm_output, _ = self.lstm(cnn_features)  # [B, T, hidden_size]\n",
    "        \n",
    "        # Prosodic Branch: Dense feature processing\n",
    "        prosodic_processed = self.prosody_branch(prosodic_features)  # [B, 64]\n",
    "        \n",
    "        # Prosody-aware attention mechanism\n",
    "        attended_output = self.prosody_aware_attention(lstm_output, prosodic_processed)\n",
    "        \n",
    "        # Global average pooling over time dimension\n",
    "        final_mfcc = attended_output.mean(dim=1)  # [B, hidden_size]\n",
    "        \n",
    "        # Feature fusion: Combine MFCC and prosodic features\n",
    "        combined_features = torch.cat([final_mfcc, prosodic_processed], dim=1)\n",
    "        \n",
    "        # Classification\n",
    "        output = self.classifier(combined_features)\n",
    "        return output\n",
    "\n",
    "# Model configuration\n",
    "mfcc_freq_dim = X_train_mfcc.shape[2] if len(X_train_mfcc.shape) > 2 else X_train_mfcc.shape[1]\n",
    "prosody_dim = X_train_prosodic.shape[1]\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "print(f\"\\nModel Configuration:\")\n",
    "print(f\"MFCC frequency dimension: {mfcc_freq_dim}\")\n",
    "print(f\"Prosodic features: {prosody_dim}\")\n",
    "print(f\"Number of emotion classes: {num_classes}\")\n",
    "\n",
    "# Initialize the Multi-Branch CRNN model\n",
    "model = MultiBranchCRNN(\n",
    "    mfcc_freq_dim=mfcc_freq_dim,\n",
    "    prosody_dim=prosody_dim,\n",
    "    num_classes=num_classes,\n",
    "    cnn_channels=[64, 128, 256],  # Progressive CNN channel increase\n",
    "    rnn_hidden_size=256,          # LSTM hidden size\n",
    "    rnn_layers=2,                 # Number of LSTM layers\n",
    "    dropout=0.3                   # Dropout for regularization\n",
    ").to(device)\n",
    "\n",
    "# Count model parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nModel Statistics:\")\n",
    "print(f\"Total parameters: {total_params:,} (~{total_params/1e6:.1f}M)\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model size estimate: ~{total_params * 4 / 1024**2:.1f} MB\")\n",
    "print(f\"Device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7538da58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "# Training Configuration for RTX 4070 Laptop\n",
    "print(\"Setting up training configuration for RTX 4070 laptop...\")\n",
    "\n",
    "# Hyperparameters optimized for RTX 4070 laptop\n",
    "BATCH_SIZE = 32              # Balanced for 8GB VRAM\n",
    "LEARNING_RATE = 1e-3         # Initial learning rate\n",
    "EPOCHS = 100                 # Maximum epochs for thorough training\n",
    "PATIENCE = 15                # Early stopping patience (increased for longer training)\n",
    "WEIGHT_DECAY = 1e-4          # L2 regularization\n",
    "GRADIENT_CLIP = 1.0          # Gradient clipping for stability\n",
    "WARMUP_EPOCHS = 5            # Learning rate warmup\n",
    "\n",
    "# Create validation split from training data\n",
    "print(\"Creating train/validation split...\")\n",
    "X_train_mfcc_split, X_val_mfcc, X_train_prosodic_split, X_val_prosodic, y_train_split, y_val = train_test_split(\n",
    "    X_train_mfcc, X_train_prosodic, y_train, \n",
    "    test_size=0.15, \n",
    "    random_state=42, \n",
    "    stratify=y_train\n",
    ")\n",
    "\n",
    "# Convert all data to tensors\n",
    "train_mfcc_tensor = torch.FloatTensor(X_train_mfcc_split)\n",
    "train_prosodic_tensor = torch.FloatTensor(X_train_prosodic_split)\n",
    "train_labels_tensor = torch.LongTensor(y_train_split)\n",
    "\n",
    "val_mfcc_tensor = torch.FloatTensor(X_val_mfcc)\n",
    "val_prosodic_tensor = torch.FloatTensor(X_val_prosodic)\n",
    "val_labels_tensor = torch.LongTensor(y_val)\n",
    "\n",
    "test_mfcc_tensor = torch.FloatTensor(X_test_mfcc)\n",
    "test_prosodic_tensor = torch.FloatTensor(X_test_prosodic)\n",
    "test_labels_tensor = torch.LongTensor(y_test)\n",
    "\n",
    "# Create datasets and data loaders\n",
    "train_dataset = TensorDataset(train_mfcc_tensor, train_prosodic_tensor, train_labels_tensor)\n",
    "val_dataset = TensorDataset(val_mfcc_tensor, val_prosodic_tensor, val_labels_tensor)\n",
    "test_dataset = TensorDataset(test_mfcc_tensor, test_prosodic_tensor, test_labels_tensor)\n",
    "\n",
    "# Data loaders with optimized settings for RTX 4070\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=4,      # Optimized for modern CPUs\n",
    "    pin_memory=True,    # Faster GPU transfer\n",
    "    persistent_workers=True  # Reuse workers\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=4, \n",
    "    pin_memory=True,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=4, \n",
    "    pin_memory=True,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "# Optimizer with weight decay for regularization\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=LEARNING_RATE, \n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-8\n",
    ")\n",
    "\n",
    "# Learning rate scheduler with warmup and decay\n",
    "def get_lr_scheduler(optimizer, warmup_epochs, total_epochs):\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch < warmup_epochs:\n",
    "            # Linear warmup\n",
    "            return epoch / warmup_epochs\n",
    "        else:\n",
    "            # Cosine annealing after warmup\n",
    "            return 0.5 * (1 + math.cos(math.pi * (epoch - warmup_epochs) / (total_epochs - warmup_epochs)))\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "scheduler = get_lr_scheduler(optimizer, WARMUP_EPOCHS, EPOCHS)\n",
    "\n",
    "# Loss function with label smoothing for better generalization\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "# Mixed precision scaler for RTX 4070 efficiency\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Training batches per epoch: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "\n",
    "print(f\"\\nTraining Configuration:\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Weight decay: {WEIGHT_DECAY}\")\n",
    "print(f\"Max epochs: {EPOCHS}\")\n",
    "print(f\"Early stopping patience: {PATIENCE}\")\n",
    "print(f\"Gradient clipping: {GRADIENT_CLIP}\")\n",
    "print(f\"Mixed precision: Enabled\")\n",
    "print(\"Configuration complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65ebe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgressTracker:\n",
    "    \"\"\"Real-time training progress tracker\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.train_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_losses = []\n",
    "        self.val_accuracies = []\n",
    "        self.learning_rates = []\n",
    "        self.best_val_acc = 0.0\n",
    "        self.best_epoch = 0\n",
    "        self.patience_counter = 0\n",
    "    \n",
    "    def update(self, train_loss, train_acc, val_loss, val_acc, lr, epoch):\n",
    "        self.train_losses.append(train_loss)\n",
    "        self.train_accuracies.append(train_acc)\n",
    "        self.val_losses.append(val_loss)\n",
    "        self.val_accuracies.append(val_acc)\n",
    "        self.learning_rates.append(lr)\n",
    "        \n",
    "        if val_acc > self.best_val_acc:\n",
    "            self.best_val_acc = val_acc\n",
    "            self.best_epoch = epoch\n",
    "            self.patience_counter = 0\n",
    "            return True  # New best model\n",
    "        else:\n",
    "            self.patience_counter += 1\n",
    "            return False\n",
    "    \n",
    "    def should_stop(self, patience):\n",
    "        return self.patience_counter >= patience\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, scaler, device):\n",
    "    \"\"\"Train model for one epoch with progress tracking\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (mfcc, prosodic, labels) in enumerate(train_loader):\n",
    "        # Move data to GPU\n",
    "        mfcc, prosodic, labels = mfcc.to(device), prosodic.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass with mixed precision\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(mfcc, prosodic)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass with gradient scaling\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # Gradient clipping for stability\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP)\n",
    "        \n",
    "        # Optimizer step\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # Progress update every 10 batches\n",
    "        if batch_idx % 10 == 0:\n",
    "            current_acc = 100. * correct / total\n",
    "            print(f'  Batch {batch_idx:3d}/{len(train_loader)} | '\n",
    "                  f'Loss: {loss.item():.4f} | Acc: {current_acc:5.2f}% | '\n",
    "                  f'GPU Mem: {torch.cuda.memory_allocated()/1024**3:.1f}GB', end='\\r')\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate_epoch(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate model performance\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for mfcc, prosodic, labels in val_loader:\n",
    "            mfcc, prosodic, labels = mfcc.to(device), prosodic.to(device), labels.to(device)\n",
    "            \n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(mfcc, prosodic)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(val_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "# Initialize progress tracker\n",
    "tracker = ProgressTracker()\n",
    "best_model_state = None\n",
    "\n",
    "print(\"Starting Multi-Branch CRNN Training...\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Target: >80% accuracy on speech emotion recognition\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Model: Multi-Branch CRNN with Prosody-Aware Attention\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    print(f'\\nEpoch {epoch+1:2d}/{EPOCHS}:')\n",
    "    \n",
    "    # Training phase\n",
    "    print('  Training...')\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, scaler, device)\n",
    "    \n",
    "    # Validation phase\n",
    "    print('\\n  Validating...')\n",
    "    val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Learning rate step\n",
    "    scheduler.step()\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Update progress tracker\n",
    "    is_best = tracker.update(train_loss, train_acc, val_loss, val_acc, current_lr, epoch)\n",
    "    \n",
    "    # Save best model\n",
    "    if is_best:\n",
    "        best_model_state = model.state_dict().copy()\n",
    "        improvement = \"NEW BEST!\"\n",
    "    else:\n",
    "        improvement = f\"({tracker.patience_counter}/{PATIENCE})\"\n",
    "    \n",
    "    # Epoch summary\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    print(f'\\n  Results:')\n",
    "    print(f'    Train Loss: {train_loss:.4f} | Train Acc: {train_acc:5.2f}%')\n",
    "    print(f'    Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:5.2f}% {improvement}')\n",
    "    print(f'    LR: {current_lr:.2e} | Time: {epoch_time:.1f}s')\n",
    "    \n",
    "    # Progress bar simulation\n",
    "    progress = (epoch + 1) / EPOCHS\n",
    "    bar_length = 30\n",
    "    filled_length = int(bar_length * progress)\n",
    "    bar = '=' * filled_length + '-' * (bar_length - filled_length)\n",
    "    print(f'  Progress: [{bar}] {progress*100:.1f}%')\n",
    "    \n",
    "    # Early stopping check\n",
    "    if tracker.should_stop(PATIENCE):\n",
    "        print(f'\\nEarly stopping triggered at epoch {epoch+1}')\n",
    "        print(f'   Best validation accuracy: {tracker.best_val_acc:.2f}% (epoch {tracker.best_epoch+1})')\n",
    "        break\n",
    "    \n",
    "    # Memory cleanup\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Load best model\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(f'\\nLoaded best model from epoch {tracker.best_epoch+1}')\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f'\\nTraining completed in {total_time/60:.1f} minutes')\n",
    "print(f'Best validation accuracy: {tracker.best_val_acc:.2f}%')\n",
    "\n",
    "# Training summary\n",
    "print(f'\\n{\"=\"*50}')\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(f'{\"=\"*50}')\n",
    "print(f'Best Validation Accuracy: {tracker.best_val_acc:.2f}%')\n",
    "print(f'Target Achievement: {\"ACHIEVED\" if tracker.best_val_acc >= 80 else \"NOT YET\"}')\n",
    "print(f'Total Training Time: {total_time/60:.1f} minutes')\n",
    "print(f'Best Epoch: {tracker.best_epoch+1}')\n",
    "print(f'Model Parameters: {total_params:,}')\n",
    "print(f'{\"=\"*50}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaf4111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, criterion, device):\n",
    "    \"\"\"Comprehensive model evaluation on test set\"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_probabilities = []\n",
    "    \n",
    "    print(\"Evaluating model on test set...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (mfcc, prosodic, labels) in enumerate(test_loader):\n",
    "            mfcc, prosodic, labels = mfcc.to(device), prosodic.to(device), labels.to(device)\n",
    "            \n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(mfcc, prosodic)\n",
    "                loss = criterion(outputs, labels)\n",
    "                probabilities = F.softmax(outputs, dim=1)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probabilities.extend(probabilities.cpu().numpy())\n",
    "            \n",
    "            # Progress indicator\n",
    "            if batch_idx % 5 == 0:\n",
    "                print(f'  Test batch {batch_idx:2d}/{len(test_loader)}', end='\\r')\n",
    "    \n",
    "    test_loss = test_loss / len(test_loader)\n",
    "    test_acc = 100. * correct / total\n",
    "    \n",
    "    return test_loss, test_acc, all_predictions, all_labels, all_probabilities\n",
    "\n",
    "# Evaluate the trained model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_loss, test_acc, test_predictions, test_labels, test_probabilities = evaluate_model(\n",
    "    model, test_loader, criterion, device\n",
    ")\n",
    "\n",
    "# Emotion class names (RAVDESS dataset)\n",
    "emotion_names = [\n",
    "    'Neutral', 'Calm', 'Happy', 'Sad', \n",
    "    'Angry', 'Fearful', 'Disgust', 'Surprised'\n",
    "]\n",
    "\n",
    "print(f\"\\nTest Set Results:\")\n",
    "print(f\"   Test Loss: {test_loss:.4f}\")\n",
    "print(f\"   Test Accuracy: {test_acc:.2f}%\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(f\"\\nDetailed Classification Report:\")\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(\n",
    "    test_labels, test_predictions, \n",
    "    target_names=emotion_names, \n",
    "    digits=3\n",
    "))\n",
    "\n",
    "# Per-class accuracy analysis\n",
    "print(f\"\\nPer-Class Accuracy:\")\n",
    "class_accuracies = {}\n",
    "for i, emotion in enumerate(emotion_names):\n",
    "    mask = np.array(test_labels) == i\n",
    "    if mask.sum() > 0:\n",
    "        class_acc = (np.array(test_predictions)[mask] == i).mean() * 100\n",
    "        class_accuracies[emotion] = class_acc\n",
    "        status = \"GOOD\" if class_acc >= 70 else \"FAIR\" if class_acc >= 50 else \"POOR\"\n",
    "        print(f\"   {emotion:10s}: {class_acc:5.1f}% {status}\")\n",
    "\n",
    "# Target achievement check\n",
    "print(f\"\\nTARGET ACHIEVEMENT:\")\n",
    "if test_acc >= 80.0:\n",
    "    print(f\"   SUCCESS! Test accuracy {test_acc:.2f}% >= 80% target\")\n",
    "    achievement = \"ACHIEVED\"\n",
    "else:\n",
    "    print(f\"   Progress: {test_acc:.2f}% (target: 80%)\")\n",
    "    print(f\"   Gap to target: {80.0 - test_acc:.2f}%\")\n",
    "    achievement = \"IN PROGRESS\"\n",
    "\n",
    "# Model performance summary\n",
    "print(f\"\\nPerformance Summary:\")\n",
    "print(f\"   Best Validation Accuracy: {tracker.best_val_acc:.2f}%\")\n",
    "print(f\"   Final Test Accuracy: {test_acc:.2f}%\")\n",
    "print(f\"   Generalization Gap: {tracker.best_val_acc - test_acc:.2f}%\")\n",
    "print(f\"   Model Complexity: {total_params:,} parameters\")\n",
    "print(f\"   Training Efficiency: {total_time/60:.1f} minutes\")\n",
    "print(f\"   Target Status: {achievement}\")\n",
    "\n",
    "# Architecture effectiveness analysis\n",
    "print(f\"\\nArchitecture Analysis:\")\n",
    "print(f\"   Multi-Branch Design: MFCC CNN + Prosodic Dense\")\n",
    "print(f\"   Attention Mechanism: Prosody-Aware Multi-Head\")\n",
    "print(f\"   Temporal Modeling: Bidirectional LSTM\")\n",
    "print(f\"   Regularization: Dropout + Label Smoothing + Weight Decay\")\n",
    "print(f\"   Optimization: Mixed Precision + Gradient Clipping\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d682721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results Visualization and Model Saving\n",
    "import os\n",
    "\n",
    "print(\"Creating visualizations...\")\n",
    "\n",
    "# Create visualization figure\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Multi-Branch CRNN Training Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Training and Validation Loss\n",
    "ax1.plot(tracker.train_losses, label='Training Loss', color='#2E86AB', linewidth=2)\n",
    "ax1.plot(tracker.val_losses, label='Validation Loss', color='#A23B72', linewidth=2)\n",
    "ax1.set_title('Model Loss Over Time', fontweight='bold')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim(bottom=0)\n",
    "\n",
    "# 2. Training and Validation Accuracy\n",
    "ax2.plot(tracker.train_accuracies, label='Training Accuracy', color='#F18F01', linewidth=2)\n",
    "ax2.plot(tracker.val_accuracies, label='Validation Accuracy', color='#C73E1D', linewidth=2)\n",
    "ax2.axhline(y=80, color='green', linestyle='--', alpha=0.7, label='Target (80%)')\n",
    "ax2.set_title('Model Accuracy Over Time', fontweight='bold')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim(0, 100)\n",
    "\n",
    "# 3. Confusion Matrix\n",
    "cm = confusion_matrix(test_labels, test_predictions)\n",
    "im = ax3.imshow(cm, interpolation='nearest', cmap='Blues')\n",
    "ax3.set_title('Test Set Confusion Matrix', fontweight='bold')\n",
    "ax3.set_xlabel('Predicted Emotion')\n",
    "ax3.set_ylabel('True Emotion')\n",
    "\n",
    "# Add text annotations to confusion matrix\n",
    "thresh = cm.max() / 2.\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax3.text(j, i, format(cm[i, j], 'd'),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "                fontweight='bold')\n",
    "\n",
    "# Set tick labels\n",
    "tick_marks = np.arange(len(emotion_names))\n",
    "ax3.set_xticks(tick_marks)\n",
    "ax3.set_yticks(tick_marks)\n",
    "ax3.set_xticklabels([name[:4] for name in emotion_names], rotation=45)\n",
    "ax3.set_yticklabels([name[:4] for name in emotion_names])\n",
    "\n",
    "# 4. Per-Class Accuracy Bar Chart\n",
    "emotions = list(class_accuracies.keys())\n",
    "accuracies = list(class_accuracies.values())\n",
    "colors = ['#28a745' if acc >= 70 else '#ffc107' if acc >= 50 else '#dc3545' for acc in accuracies]\n",
    "\n",
    "bars = ax4.bar(emotions, accuracies, color=colors, alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "ax4.axhline(y=80, color='green', linestyle='--', alpha=0.7, label='Target (80%)')\n",
    "ax4.set_title('Per-Class Test Accuracy', fontweight='bold')\n",
    "ax4.set_xlabel('Emotion Class')\n",
    "ax4.set_ylabel('Accuracy (%)')\n",
    "ax4.set_ylim(0, 100)\n",
    "ax4.tick_params(axis='x', rotation=45)\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "             f'{acc:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the trained model\n",
    "print(\"\\nSaving trained model...\")\n",
    "os.makedirs('models', exist_ok=True)\n",
    "model_path = 'models/multibranch_crnn_emotion_model.pth'\n",
    "\n",
    "# Comprehensive model save with all important information\n",
    "save_dict = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_config': {\n",
    "        'mfcc_freq_dim': mfcc_freq_dim,\n",
    "        'prosody_dim': prosody_dim,\n",
    "        'num_classes': num_classes,\n",
    "        'cnn_channels': [64, 128, 256],\n",
    "        'rnn_hidden_size': 256,\n",
    "        'rnn_layers': 2,\n",
    "        'dropout': 0.3\n",
    "    },\n",
    "    'training_config': {\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'weight_decay': WEIGHT_DECAY,\n",
    "        'epochs_trained': len(tracker.train_losses),\n",
    "        'early_stopping_patience': PATIENCE\n",
    "    },\n",
    "    'results': {\n",
    "        'best_val_accuracy': tracker.best_val_acc,\n",
    "        'best_epoch': tracker.best_epoch,\n",
    "        'test_accuracy': test_acc,\n",
    "        'test_loss': test_loss,\n",
    "        'total_parameters': total_params,\n",
    "        'training_time_minutes': total_time/60\n",
    "    },\n",
    "    'training_history': {\n",
    "        'train_losses': tracker.train_losses,\n",
    "        'val_losses': tracker.val_losses,\n",
    "        'train_accuracies': tracker.train_accuracies,\n",
    "        'val_accuracies': tracker.val_accuracies,\n",
    "        'learning_rates': tracker.learning_rates\n",
    "    },\n",
    "    'evaluation': {\n",
    "        'emotion_names': emotion_names,\n",
    "        'class_accuracies': class_accuracies,\n",
    "        'confusion_matrix': cm.tolist(),\n",
    "        'test_predictions': test_predictions,\n",
    "        'test_labels': test_labels\n",
    "    },\n",
    "    'metadata': {\n",
    "        'pytorch_version': torch.__version__,\n",
    "        'device': str(device),\n",
    "        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'architecture': 'Multi-Branch CRNN with Prosody-Aware Attention'\n",
    "    }\n",
    "}\n",
    "\n",
    "torch.save(save_dict, model_path)\n",
    "\n",
    "model_size_mb = os.path.getsize(model_path) / (1024**2)\n",
    "print(f\"Model saved successfully!\")\n",
    "print(f\"   Path: {model_path}\")\n",
    "print(f\"   Size: {model_size_mb:.1f} MB\")\n",
    "\n",
    "# Final Results Summary\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"MULTI-BRANCH CRNN TRAINING COMPLETE\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Final Test Accuracy: {test_acc:.2f}%\")\n",
    "print(f\"Best Validation Accuracy: {tracker.best_val_acc:.2f}%\")\n",
    "print(f\"Target Achievement: {'ACHIEVED!' if test_acc >= 80 else 'In Progress'}\")\n",
    "print(f\"Architecture: Multi-Branch CRNN + Prosody-Aware Attention\")\n",
    "print(f\"Training Time: {total_time/60:.1f} minutes\")\n",
    "print(f\"Model Parameters: {total_params:,}\")\n",
    "print(f\"Model Saved: {model_path}\")\n",
    "print(f\"Optimized for: RTX 4070 Laptop\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "if test_acc >= 80:\n",
    "    print(\"Congratulations! Your model has achieved the target accuracy!\")\n",
    "    print(\"Ready for deployment in speech emotion recognition applications.\")\n",
    "else:\n",
    "    print(\"To improve performance, consider:\")\n",
    "    print(\"   • Increasing model complexity or training epochs\")\n",
    "    print(\"   • Fine-tuning hyperparameters\")\n",
    "    print(\"   • Data augmentation techniques\")\n",
    "    print(\"   • Ensemble methods\")\n",
    "    \n",
    "print(\"\\nModel Architecture Summary:\")\n",
    "print(\"   MFCC Branch: CNN (64→128→256) + Bidirectional LSTM\")\n",
    "print(\"   Prosodic Branch: Dense layers (128→64)\")\n",
    "print(\"   Attention: Prosody-Aware Multi-Head (8 heads)\")\n",
    "print(\"   Fusion: Feature concatenation + Dense classifier\")\n",
    "print(\"   Optimization: Mixed precision + Gradient clipping + Early stopping\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
